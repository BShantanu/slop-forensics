{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0vW0ORTHugjL",
      "metadata": {
        "id": "0vW0ORTHugjL"
      },
      "source": [
        "Slop Forensics Toolkit - Colab Runner\n",
        "\n",
        "Created by Sam Paech\n",
        "\n",
        "Code repository: https://github.com/sam-paech/slop-forensics\n",
        "\n",
        "This notebook downloads the Slop Forensics toolkit, installs dependencies,\n",
        "and runs through the main workflow steps as described in the README.\n",
        "\n",
        "**Requires:**\n",
        "- An OpenAI-compatible base url & API Key (e.g., from OpenRouter)\n",
        "- Several dependencies (installed in the first cell of the notebook)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1_Setup",
      "metadata": {
        "accelerator": "GPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_Setup",
        "outputId": "43122fe8-3916-4032-89f0-381fdf3bdce6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reset to clean directory: /content\n",
            "Updating package list...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\n",
            "Repository directory 'slop-forensics' already exists. Skipping clone.\n",
            "Pulling latest changes...\n",
            "Already up to date.\n",
            "Added /content to sys.path\n",
            "\n",
            "Changing directory to slop-forensics...\n",
            "Current working directory: /content/slop-forensics\n",
            "\n",
            "Installing Python dependencies from requirements.txt...\n",
            "\n",
            "Installing system dependencies (PHYLIP)...\n",
            "\n",
            "Verifying PHYLIP installation (checking for 'pars')...\n",
            "WARN: PHYLIP 'pars' command not found in PATH after installation.\n",
            "Phylogenetic tree generation using parsimony might fail.\n",
            "The script might fall back to hierarchical clustering.\n",
            "\n",
            "Downloading required NLTK data...\n",
            "NLTK data downloaded successfully.\n",
            "\n",
            "Setup complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Setup Environment and Dependencies\n",
        "# @markdown Clone the repository, install Python requirements, system dependencies (PHYLIP), and NLTK data.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import nltk\n",
        "from google.colab import userdata # For secrets\n",
        "\n",
        "# Reset to a known location first\n",
        "if 'COLAB_GPU' in os.environ:  # Check if we're in Colab\n",
        "    # Reset to Colab root directory\n",
        "    os.chdir('/content')\n",
        "else:\n",
        "    # For local environments, try to find the root of the repository\n",
        "    current_dir = os.getcwd()\n",
        "    # Go up until we're out of any nested slop-forensics directories\n",
        "    while 'slop-forensics' in current_dir and current_dir != os.path.dirname(current_dir):\n",
        "        current_dir = os.path.dirname(current_dir)\n",
        "    os.chdir(current_dir)\n",
        "\n",
        "print(f\"Reset to clean directory: {os.getcwd()}\")\n",
        "\n",
        "# --- Configuration ---\n",
        "GIT_REPO_URL = \"https://github.com/sam-paech/slop-forensics.git\"\n",
        "REPO_DIR = \"slop-forensics\"\n",
        "# --- End Configuration ---\n",
        "\n",
        "print(\"Updating package list...\")\n",
        "!sudo apt-get update -qq\n",
        "\n",
        "# Check if the repository already exists\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    print(f\"\\nCloning the repository to {REPO_DIR}...\")\n",
        "    !git clone {GIT_REPO_URL}\n",
        "else:\n",
        "    print(f\"\\nRepository directory '{REPO_DIR}' already exists. Skipping clone.\")\n",
        "    # Optionally pull latest changes if it's a git repository\n",
        "    if os.path.exists(os.path.join(REPO_DIR, \".git\")):\n",
        "        print(\"Pulling latest changes...\")\n",
        "        !cd {REPO_DIR} && git pull\n",
        "\n",
        "# Add project root to Python path for imports within scripts\n",
        "project_root = os.path.abspath(os.getcwd())\n",
        "sys.path.insert(0, project_root)\n",
        "print(f\"Added {project_root} to sys.path\")\n",
        "\n",
        "# Change directory into the repository (only if not already in it)\n",
        "current_dir = os.getcwd()\n",
        "target_dir = os.path.join(project_root, REPO_DIR)\n",
        "\n",
        "if current_dir != target_dir:\n",
        "    print(f\"\\nChanging directory to {REPO_DIR}...\")\n",
        "    os.chdir(target_dir)\n",
        "    print(f\"Current working directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(f\"\\nAlready in the correct directory: {current_dir}\")\n",
        "\n",
        "print(\"\\nInstalling Python dependencies from requirements.txt...\")\n",
        "# Using -q for quieter output, remove if you want to see all details\n",
        "!pip install -q -r requirements.txt\n",
        "\n",
        "print(\"\\nInstalling system dependencies (PHYLIP)...\")\n",
        "# Using -qq for quieter output, remove if you want to see all details\n",
        "# Using -y to automatically confirm installation\n",
        "!sudo apt-get install -y -qq phylip\n",
        "\n",
        "# Verify PHYLIP installation (optional)\n",
        "print(\"\\nVerifying PHYLIP installation (checking for 'pars')...\")\n",
        "try:\n",
        "    result = subprocess.run(['which', 'pars'], capture_output=True, text=True, check=True)\n",
        "    print(f\"PHYLIP 'pars' found at: {result.stdout.strip()}\")\n",
        "    PHYLIP_INSTALLED = True\n",
        "    # PHYLIP is usually installed in /usr/local/bin, which should be in PATH\n",
        "    PHYLIP_PATH = \"/usr/local/bin\"\n",
        "except subprocess.CalledProcessError:\n",
        "    print(\"WARN: PHYLIP 'pars' command not found in PATH after installation.\")\n",
        "    print(\"Phylogenetic tree generation using parsimony might fail.\")\n",
        "    print(\"The script might fall back to hierarchical clustering.\")\n",
        "    PHYLIP_INSTALLED = False\n",
        "    PHYLIP_PATH = \"/usr/local/bin\" # Let the script try the default PATH\n",
        "\n",
        "print(\"\\nDownloading required NLTK data...\")\n",
        "try:\n",
        "    nltk.download('punkt', quiet=False)\n",
        "    nltk.download('punkt_tab', quiet=False)\n",
        "    nltk.download('stopwords', quiet=False)\n",
        "    nltk.download('cmudict', quiet=False)\n",
        "    print(\"NLTK data downloaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading NLTK data: {e}\")\n",
        "    print(\"Some analysis features might be limited.\")\n",
        "\n",
        "print(\"\\nSetup complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2_Configure",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_Configure",
        "outputId": "eabb3ac6-e073-4621-8e9e-3270b1699bd5"
      },
      "outputs": [],
      "source": [
        "# @title 2. Configure API Key and Environment\n",
        "# @markdown Create the `.env` file using Colab Secrets.\n",
        "\n",
        "# --- Configuration ---\n",
        "# Models to run the generation step for (comma-separated string)\n",
        "# Using a small set for demonstration purposes. Add more as needed.\n",
        "# Ensure these models are available via your API provider (e.g., OpenRouter)\n",
        "MODELS_TO_RUN = \"x-ai/grok-3-mini-beta,meta-llama/llama-4-maverick,meta-llama/llama-4-scout,google/gemma-3-4b-it\"\n",
        "\n",
        "# Number of records to generate per model (use a small number for testing)\n",
        "NUM_GENERATIONS = 100 # Default in repo is 1000, use 5-10 for quick test\n",
        "\n",
        "# --- End Configuration ---\n",
        "\n",
        "\n",
        "# --- API Config ---\n",
        "# This code assumes you have an OpenAI compatible API ready to go.\n",
        "# You could use openrouter, or vllm/llama.cpp etc for local models.\n",
        "base_url=\"https://openrouter.ai/api/v1\"\n",
        "api_key=\"YOUR_API_KEY\"\n",
        "\n",
        "# --- Create .env file ---\n",
        "env_content = f\"\"\"\n",
        "# OpenAI Compatible API Key (or other LLM provider)\n",
        "OPENAI_API_KEY=\\\"{api_key}\\\"\n",
        "OPENAI_BASE_URL=\\\"{base_url}\\\"\n",
        "\"\"\"\n",
        "try:\n",
        "    with open(\".env\", \"w\") as f:\n",
        "        f.write(env_content.strip())\n",
        "    print(\"\\nSuccessfully created .env file:\")\n",
        "    !cat .env\n",
        "except IOError as e:\n",
        "    print(f\"\\nError writing .env file: {e}\")\n",
        "\n",
        "# --- Set Environment Variable for ETE3 ---\n",
        "# ETE3 uses Qt for visualization, which can cause issues in headless environments.\n",
        "# The phylogeny script already does this, but setting it here ensures it's done early.\n",
        "os.environ[\"QT_QPA_PLATFORM\"] = \"offscreen\"\n",
        "print(\"\\nSet QT_QPA_PLATFORM=offscreen for ETE3.\")\n",
        "\n",
        "print(\"\\nConfiguration complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "3_Generate",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_Generate",
        "outputId": "8b97f072-9f69-4daa-c0ce-300b1289cb3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting dataset generation for 100 records per model...\n",
            "Models: x-ai/grok-3-mini-beta,meta-llama/llama-4-maverick,meta-llama/llama-4-scout,google/gemma-3-4b-it\n",
            "\n",
            "Executing command:\n",
            "\n",
            "python3 scripts/generate_dataset.py   --model-ids \"x-ai/grok-3-mini-beta,meta-llama/llama-4-maverick,meta-llama/llama-4-scout,google/gemma-3-4b-it\"   --generate-n 100\n",
            "\n",
            "2025-04-10 09:19:29,565 - INFO - generate_dataset - Starting dataset generation for models: x-ai/grok-3-mini-beta, meta-llama/llama-4-maverick, meta-llama/llama-4-scout, google/gemma-3-4b-it\n",
            "2025-04-10 09:19:29,565 - INFO - generate_dataset - Output directory: /content/slop-forensics/results/datasets\n",
            "2025-04-10 09:19:29,565 - INFO - generate_dataset - Target records per model: 100\n",
            "2025-04-10 09:19:29,565 - INFO - dataset_generator - Starting generation process for model: x-ai/grok-3-mini-beta\n",
            "2025-04-10 09:19:29,565 - INFO - dataset_generator - Loading dataset: Nitral-AI/Reddit-SFW-Writing_Prompts_ShareGPT (Source: Nitral-AI)\n",
            "2025-04-10 09:19:32,055 - INFO - dataset_generator - Loaded 177477 rows from Nitral-AI.\n",
            "Processing Nitral-AI: 100% 177477/177477 [00:08<00:00, 21482.86prompts/s]\n",
            "2025-04-10 09:19:40,318 - INFO - dataset_generator - Total prompts loaded across sources: 177477 (before filtering/resume)\n",
            "2025-04-10 09:19:40,318 - INFO - dataset_generator - Total prompts to process (after filtering/resume): 177477\n",
            "2025-04-10 09:19:40,389 - INFO - dataset_generator - Need 100 more records. Will process 100 available prompts.\n",
            "2025-04-10 09:19:40,389 - INFO - dataset_generator - Initializing ThreadPoolExecutor with 500 workers.\n",
            "2025-04-10 09:19:41,149 - INFO - dataset_generator - Submitted 100 tasks to the executor for x-ai/grok-3-mini-beta.\n",
            "Generating (x-ai/grok-3-mini-beta):  19% 19/100 [00:12<00:12,  6.30prompt/s, saved_total=19, processed_session=19]2025-04-10 09:19:53,399 - INFO - dataset_generator - Buffer full (20 items). Saving batch...\n",
            "Generating (x-ai/grok-3-mini-beta):  39% 39/100 [00:12<00:02, 23.38prompt/s, saved_total=39, processed_session=39]2025-04-10 09:19:54,023 - INFO - dataset_generator - Buffer full (20 items). Saving batch...\n",
            "Generating (x-ai/grok-3-mini-beta):  60% 60/100 [00:13<00:01, 35.68prompt/s, saved_total=59, processed_session=59]2025-04-10 09:19:54,569 - INFO - dataset_generator - Buffer full (20 items). Saving batch...\n",
            "Generating (x-ai/grok-3-mini-beta):  79% 79/100 [00:14<00:00, 29.27prompt/s, saved_total=79, processed_session=79]2025-04-10 09:19:55,341 - INFO - dataset_generator - Buffer full (20 items). Saving batch...\n",
            "Generating (x-ai/grok-3-mini-beta): 100% 100/100 [00:16<00:00,  4.91prompt/s, saved_total=99, processed_session=99]2025-04-10 09:19:57,968 - INFO - dataset_generator - Buffer full (20 items). Saving batch...\n",
            "Generating (x-ai/grok-3-mini-beta): 100% 100/100 [00:16<00:00,  4.91prompt/s, saved_total=100, processed_session=100]2025-04-10 09:19:57,970 - INFO - dataset_generator - Target of 100 records reached for x-ai/grok-3-mini-beta. Stopping processing.\n",
            "Generating (x-ai/grok-3-mini-beta): 100% 100/100 [00:16<00:00,  5.94prompt/s, saved_total=100, processed_session=100]\n",
            "2025-04-10 09:19:57,971 - INFO - dataset_generator - Generation process finished for x-ai/grok-3-mini-beta. Prompts processed in this run: 100. Results saved in this run: 100. Total results saved: 100.\n",
            "2025-04-10 09:19:57,980 - INFO - dataset_generator - Starting generation process for model: meta-llama/llama-4-maverick\n",
            "2025-04-10 09:19:57,980 - INFO - dataset_generator - Loading dataset: Nitral-AI/Reddit-SFW-Writing_Prompts_ShareGPT (Source: Nitral-AI)\n",
            "2025-04-10 09:19:58,716 - INFO - dataset_generator - Loaded 177477 rows from Nitral-AI.\n",
            "Processing Nitral-AI: 100% 177477/177477 [00:08<00:00, 20390.54prompts/s]\n",
            "2025-04-10 09:20:07,420 - INFO - dataset_generator - Total prompts loaded across sources: 177477 (before filtering/resume)\n",
            "2025-04-10 09:20:07,420 - INFO - dataset_generator - Total prompts to process (after filtering/resume): 177477\n",
            "2025-04-10 09:20:07,468 - INFO - dataset_generator - Need 100 more records. Will process 100 available prompts.\n",
            "2025-04-10 09:20:07,468 - INFO - dataset_generator - Initializing ThreadPoolExecutor with 500 workers.\n",
            "2025-04-10 09:20:07,978 - INFO - dataset_generator - Submitted 100 tasks to the executor for meta-llama/llama-4-maverick.\n",
            "Generating (meta-llama/llama-4-maverick):  20% 20/100 [00:04<00:07, 10.26prompt/s, saved_total=19, processed_session=19]2025-04-10 09:20:12,355 - INFO - dataset_generator - Buffer full (20 items). Saving batch...\n",
            "Generating (meta-llama/llama-4-maverick):  39% 39/100 [00:09<00:09,  6.74prompt/s, saved_total=39, processed_session=39]2025-04-10 09:20:17,368 - INFO - dataset_generator - Buffer full (20 items). Saving batch...\n",
            "Generating (meta-llama/llama-4-maverick):  59% 59/100 [00:10<00:03, 11.42prompt/s, saved_total=59, processed_session=59]2025-04-10 09:20:18,921 - INFO - dataset_generator - Buffer full (20 items). Saving batch...\n",
            "Generating (meta-llama/llama-4-maverick):  79% 79/100 [00:12<00:01, 15.28prompt/s, saved_total=79, processed_session=79]2025-04-10 09:20:20,400 - INFO - dataset_generator - Buffer full (20 items). Saving batch...\n",
            "Generating (meta-llama/llama-4-maverick): 100% 100/100 [00:15<00:00,  6.46prompt/s, saved_total=99, processed_session=99]2025-04-10 09:20:23,140 - INFO - dataset_generator - Buffer full (20 items). Saving batch...\n",
            "Generating (meta-llama/llama-4-maverick): 100% 100/100 [00:15<00:00,  6.46prompt/s, saved_total=100, processed_session=100]2025-04-10 09:20:23,142 - INFO - dataset_generator - Target of 100 records reached for meta-llama/llama-4-maverick. Stopping processing.\n",
            "Generating (meta-llama/llama-4-maverick): 100% 100/100 [00:15<00:00,  6.59prompt/s, saved_total=100, processed_session=100]\n",
            "2025-04-10 09:20:23,143 - INFO - dataset_generator - Generation process finished for meta-llama/llama-4-maverick. Prompts processed in this run: 100. Results saved in this run: 100. Total results saved: 100.\n",
            "2025-04-10 09:20:23,151 - INFO - dataset_generator - Starting generation process for model: meta-llama/llama-4-scout\n",
            "2025-04-10 09:20:23,151 - INFO - dataset_generator - Loading dataset: Nitral-AI/Reddit-SFW-Writing_Prompts_ShareGPT (Source: Nitral-AI)\n",
            "2025-04-10 09:20:24,004 - INFO - dataset_generator - Loaded 177477 rows from Nitral-AI.\n",
            "Processing Nitral-AI: 100% 177477/177477 [00:08<00:00, 20142.50prompts/s]\n",
            "2025-04-10 09:20:32,816 - INFO - dataset_generator - Total prompts loaded across sources: 177477 (before filtering/resume)\n",
            "2025-04-10 09:20:32,816 - INFO - dataset_generator - Total prompts to process (after filtering/resume): 177477\n",
            "2025-04-10 09:20:32,862 - INFO - dataset_generator - Need 100 more records. Will process 100 available prompts.\n",
            "2025-04-10 09:20:32,862 - INFO - dataset_generator - Initializing ThreadPoolExecutor with 500 workers.\n",
            "2025-04-10 09:20:33,364 - INFO - dataset_generator - Submitted 100 tasks to the executor for meta-llama/llama-4-scout.\n",
            "Generating (meta-llama/llama-4-scout):  19% 19/100 [00:10<01:07,  1.20prompt/s, saved_total=19, processed_session=19]2025-04-10 09:20:43,441 - INFO - dataset_generator - Buffer full (20 items). Saving batch...\n",
            "Generating (meta-llama/llama-4-scout):  39% 39/100 [00:12<00:08,  7.35prompt/s, saved_total=39, processed_session=39]2025-04-10 09:20:46,375 - INFO - dataset_generator - Buffer full (20 items). Saving batch...\n",
            "Generating (meta-llama/llama-4-scout):  60% 60/100 [00:18<00:13,  2.95prompt/s, saved_total=59, processed_session=59]2025-04-10 09:20:52,083 - INFO - dataset_generator - Buffer full (20 items). Saving batch...\n",
            "Generating (meta-llama/llama-4-scout):  80% 80/100 [00:38<00:24,  1.20s/prompt, saved_total=79, processed_session=79]2025-04-10 09:21:11,838 - INFO - dataset_generator - Buffer full (20 items). Saving batch...\n",
            "Generating (meta-llama/llama-4-scout): 100% 100/100 [01:13<00:00,  4.85s/prompt, saved_total=99, processed_session=99]2025-04-10 09:21:47,301 - INFO - dataset_generator - Buffer full (20 items). Saving batch...\n",
            "Generating (meta-llama/llama-4-scout): 100% 100/100 [01:13<00:00,  4.85s/prompt, saved_total=100, processed_session=100]2025-04-10 09:21:47,303 - INFO - dataset_generator - Target of 100 records reached for meta-llama/llama-4-scout. Stopping processing.\n",
            "Generating (meta-llama/llama-4-scout): 100% 100/100 [01:13<00:00,  1.35prompt/s, saved_total=100, processed_session=100]\n",
            "2025-04-10 09:21:47,304 - INFO - dataset_generator - Generation process finished for meta-llama/llama-4-scout. Prompts processed in this run: 100. Results saved in this run: 100. Total results saved: 100.\n",
            "2025-04-10 09:21:47,313 - INFO - dataset_generator - Starting generation process for model: google/gemma-3-4b-it\n",
            "2025-04-10 09:21:47,313 - INFO - dataset_generator - Loading dataset: Nitral-AI/Reddit-SFW-Writing_Prompts_ShareGPT (Source: Nitral-AI)\n",
            "2025-04-10 09:21:48,107 - INFO - dataset_generator - Loaded 177477 rows from Nitral-AI.\n",
            "Processing Nitral-AI: 100% 177477/177477 [00:08<00:00, 20019.17prompts/s]\n",
            "2025-04-10 09:21:56,973 - INFO - dataset_generator - Total prompts loaded across sources: 177477 (before filtering/resume)\n",
            "2025-04-10 09:21:56,973 - INFO - dataset_generator - Total prompts to process (after filtering/resume): 177477\n",
            "2025-04-10 09:21:57,054 - INFO - dataset_generator - Need 100 more records. Will process 100 available prompts.\n",
            "2025-04-10 09:21:57,054 - INFO - dataset_generator - Initializing ThreadPoolExecutor with 500 workers.\n",
            "2025-04-10 09:21:57,624 - INFO - dataset_generator - Submitted 100 tasks to the executor for google/gemma-3-4b-it.\n",
            "Generating (google/gemma-3-4b-it):  19% 19/100 [00:30<00:13,  6.22prompt/s, saved_total=19, processed_session=19]2025-04-10 09:22:28,317 - INFO - dataset_generator - Buffer full (20 items). Saving batch...\n",
            "Generating (google/gemma-3-4b-it):  39% 39/100 [00:32<00:05, 11.06prompt/s, saved_total=39, processed_session=39]2025-04-10 09:22:29,961 - INFO - dataset_generator - Buffer full (20 items). Saving batch...\n",
            "Generating (google/gemma-3-4b-it):  60% 60/100 [00:33<00:01, 26.00prompt/s, saved_total=59, processed_session=59]2025-04-10 09:22:30,746 - INFO - dataset_generator - Buffer full (20 items). Saving batch...\n",
            "Generating (google/gemma-3-4b-it):  80% 80/100 [00:34<00:01, 12.23prompt/s, saved_total=79, processed_session=79]2025-04-10 09:22:32,262 - INFO - dataset_generator - Buffer full (20 items). Saving batch...\n",
            "Generating (google/gemma-3-4b-it): 100% 100/100 [00:37<00:00,  5.11prompt/s, saved_total=99, processed_session=99]2025-04-10 09:22:35,248 - INFO - dataset_generator - Buffer full (20 items). Saving batch...\n",
            "Generating (google/gemma-3-4b-it): 100% 100/100 [00:37<00:00,  5.11prompt/s, saved_total=100, processed_session=100]2025-04-10 09:22:35,250 - INFO - dataset_generator - Target of 100 records reached for google/gemma-3-4b-it. Stopping processing.\n",
            "Generating (google/gemma-3-4b-it): 100% 100/100 [00:37<00:00,  2.66prompt/s, saved_total=100, processed_session=100]\n",
            "2025-04-10 09:22:35,251 - INFO - dataset_generator - Generation process finished for google/gemma-3-4b-it. Prompts processed in this run: 100. Results saved in this run: 100. Total results saved: 100.\n",
            "2025-04-10 09:22:35,260 - INFO - generate_dataset - Dataset generation script finished.\n",
            "\n",
            "Dataset generation script finished.\n",
            "Check the 'results/datasets/' directory for output .jsonl files.\n",
            "total 2068\n",
            "-rw-r--r-- 1 root root 611263 Apr 10 09:22 generated_google__gemma-3-4b-it.jsonl\n",
            "-rw-r--r-- 1 root root 434105 Apr 10 09:20 generated_meta-llama__llama-4-maverick.jsonl\n",
            "-rw-r--r-- 1 root root 510781 Apr 10 09:21 generated_meta-llama__llama-4-scout.jsonl\n",
            "-rw-r--r-- 1 root root 556643 Apr 10 09:19 generated_x-ai__grok-3-mini-beta.jsonl\n"
          ]
        }
      ],
      "source": [
        "# @title 3. Run Workflow Step 1: Generate Dataset\n",
        "# @markdown Use `generate_dataset.py` to prompt the specified LLMs.\n",
        "# @markdown **Note:** This step calls the LLM API and may incur costs.\n",
        "# @markdown It can also take time depending on the number of generations and API responsiveness.\n",
        "\n",
        "print(f\"Starting dataset generation for {NUM_GENERATIONS} records per model...\")\n",
        "print(f\"Models: {MODELS_TO_RUN}\")\n",
        "\n",
        "# Construct the command\n",
        "generate_command = f\"\"\"\n",
        "python3 scripts/generate_dataset.py \\\n",
        "  --model-ids \\\"{MODELS_TO_RUN}\\\" \\\n",
        "  --generate-n {NUM_GENERATIONS}\n",
        "\"\"\"\n",
        "\n",
        "# Run the command\n",
        "print(\"\\nExecuting command:\")\n",
        "print(generate_command)\n",
        "!{generate_command}\n",
        "\n",
        "print(\"\\nDataset generation script finished.\")\n",
        "print(\"Check the 'results/datasets/' directory for output .jsonl files.\")\n",
        "!ls -l results/datasets/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "4_Analyze",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_Analyze",
        "outputId": "8925926f-f399-4b0c-ab9c-f577fb862f17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK data paths: ['/root/nltk_data', '/usr/nltk_data', '/usr/share/nltk_data', '/usr/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n",
            "Successfully loaded punkt tokenizer\n",
            "Starting analysis of generated datasets...\n",
            "\n",
            "Executing command:\n",
            "python3 scripts/slop_profile.py\n",
            "2025-04-10 09:23:28,408 - INFO - slop_profile - Starting analysis of datasets in: /content/slop-forensics/results/datasets\n",
            "2025-04-10 09:23:28,408 - INFO - slop_profile - Analysis output directory: /content/slop-forensics/results/analysis\n",
            "2025-04-10 09:23:28,408 - INFO - slop_profile - Combined metrics output file: /content/slop-forensics/results/slop_profile_results.json\n",
            "2025-04-10 09:23:28,408 - INFO - slop_profile - Max items per model: 10000\n",
            "2025-04-10 09:23:28,409 - INFO - slop_profile - Will log top 5 patterns per model\n",
            "2025-04-10 09:23:28,409 - INFO - slop_profile - Found 4 dataset files to analyze.\n",
            "2025-04-10 09:23:28,409 - WARNING - utils - File not found: /content/slop-forensics/results/slop_profile_results.json\n",
            "Analyzing Models:   0% 0/4 [00:00<?, ?it/s]2025-04-10 09:23:28,410 - INFO - slop_profile - Processing file: generated_x-ai__grok-3-mini-beta.jsonl\n",
            "2025-04-10 09:23:28,415 - INFO - slop_profile - Analyzing model: x-ai/grok-3-mini-beta (100 items)\n",
            "2025-04-10 09:23:28,416 - INFO - analysis - Starting analysis for model: x-ai/grok-3-mini-beta\n",
            "2025-04-10 09:23:32,292 - INFO - metrics - Loaded 1000 word items from data/slop_list.json\n",
            "2025-04-10 09:23:32,292 - INFO - metrics - Loaded 200 bigram items from data/slop_list_bigrams.json\n",
            "2025-04-10 09:23:32,293 - INFO - metrics - Loaded 200 trigram items from data/slop_list_trigrams.json\n",
            "2025-04-10 09:23:35,579 - INFO - analysis - Analysis complete for model: x-ai/grok-3-mini-beta\n",
            "Analyzing Models:  25% 1/4 [00:07<00:21,  7.18s/it]2025-04-10 09:23:35,594 - INFO - slop_profile - Processing file: generated_meta-llama__llama-4-scout.jsonl\n",
            "2025-04-10 09:23:35,598 - INFO - slop_profile - Analyzing model: meta-llama/llama-4-scout (100 items)\n",
            "2025-04-10 09:23:35,598 - INFO - analysis - Starting analysis for model: meta-llama/llama-4-scout\n",
            "2025-04-10 09:23:39,438 - INFO - analysis - Analysis complete for model: meta-llama/llama-4-scout\n",
            "Analyzing Models:  50% 2/4 [00:11<00:10,  5.23s/it]2025-04-10 09:23:39,452 - INFO - slop_profile - Processing file: generated_meta-llama__llama-4-maverick.jsonl\n",
            "2025-04-10 09:23:39,455 - INFO - slop_profile - Analyzing model: meta-llama/llama-4-maverick (100 items)\n",
            "2025-04-10 09:23:39,455 - INFO - analysis - Starting analysis for model: meta-llama/llama-4-maverick\n",
            "2025-04-10 09:23:42,540 - INFO - analysis - Analysis complete for model: meta-llama/llama-4-maverick\n",
            "Analyzing Models:  75% 3/4 [00:14<00:04,  4.26s/it]2025-04-10 09:23:42,554 - INFO - slop_profile - Processing file: generated_google__gemma-3-4b-it.jsonl\n",
            "2025-04-10 09:23:42,559 - INFO - slop_profile - Analyzing model: google/gemma-3-4b-it (100 items)\n",
            "2025-04-10 09:23:42,559 - INFO - analysis - Starting analysis for model: google/gemma-3-4b-it\n",
            "2025-04-10 09:23:48,302 - INFO - analysis - Analysis complete for model: google/gemma-3-4b-it\n",
            "Analyzing Models: 100% 4/4 [00:19<00:00,  4.98s/it]\n",
            "2025-04-10 09:23:48,317 - INFO - slop_profile - Saving combined metrics for 4 models to /content/slop-forensics/results/slop_profile_results.json\n",
            "2025-04-10 09:23:48,371 - INFO - slop_profile - \n",
            "========== SUMMARY OF TOP PATTERNS ==========\n",
            "2025-04-10 09:23:48,371 - INFO - slop_profile - MODEL: x-ai/grok-3-mini-beta\n",
            "2025-04-10 09:23:48,371 - INFO - slop_profile - WORDS: 'eldoria', 'zorathian', 'elara', 'shimmered', 'kael'\n",
            "2025-04-10 09:23:48,371 - INFO - slop_profile - BIGRAMS: 'said voice', 'felt like', 'heart pounding', 'first time', 'mind raced'\n",
            "2025-04-10 09:23:48,371 - INFO - slop_profile - TRIGRAMS: 'voice steady despite', 'could shake feeling', 'casting long shadows', 'said voice steady', 'words hung air'\n",
            "2025-04-10 09:23:48,371 - INFO - slop_profile - ---\n",
            "2025-04-10 09:23:48,371 - INFO - slop_profile - MODEL: meta-llama/llama-4-scout\n",
            "2025-04-10 09:23:48,371 - INFO - slop_profile - WORDS: 'zorvath', 'zrhk'tk', 'xhk'kht', 'elara', 'kaelin'\n",
            "2025-04-10 09:23:48,371 - INFO - slop_profile - BIGRAMS: 'could help', 'said voice', 'deep breath', 'took deep', 'could shake'\n",
            "2025-04-10 09:23:48,371 - INFO - slop_profile - TRIGRAMS: 'took deep breath', 'could shake feeling', 'shiver run spine', 'felt shiver run', 'voice barely whisper'\n",
            "2025-04-10 09:23:48,371 - INFO - slop_profile - ---\n",
            "2025-04-10 09:23:48,371 - INFO - slop_profile - MODEL: meta-llama/llama-4-maverick\n",
            "2025-04-10 09:23:48,371 - INFO - slop_profile - WORDS: 'zha'thik', 'zorvath', 'xeridians', 'elara', 'kael'\n",
            "2025-04-10 09:23:48,371 - INFO - slop_profile - BIGRAMS: 'said voice', 'could help', 'voice low', 'eyes fixed', 'deep breath'\n",
            "2025-04-10 09:23:48,371 - INFO - slop_profile - TRIGRAMS: 'took deep breath', 'shiver run spine', 'felt shiver run', 'voice barely whisper', 'said voice low'\n",
            "2025-04-10 09:23:48,371 - INFO - slop_profile - ---\n",
            "2025-04-10 09:23:48,371 - INFO - slop_profile - MODEL: google/gemma-3-4b-it\n",
            "2025-04-10 09:23:48,371 - INFO - slop_profile - WORDS: 'xylar', 'kryll', 'veridia', 'kaelen', 'oakhaven'\n",
            "2025-04-10 09:23:48,372 - INFO - slop_profile - BIGRAMS: 'said voice', 'flicker something', 'voice barely', 'felt like', 'carefully constructed'\n",
            "2025-04-10 09:23:48,372 - INFO - slop_profile - TRIGRAMS: 'voice barely whisper', 'rain continued fall', 'said voice barely', 'continued fall washing', 'said voice low'\n",
            "2025-04-10 09:23:48,372 - INFO - slop_profile - ---\n",
            "2025-04-10 09:23:48,372 - INFO - slop_profile - ============== END SUMMARY ===============\n",
            "2025-04-10 09:23:48,372 - INFO - slop_profile - Analysis script finished.\n",
            "2025-04-10 09:23:48,372 - INFO - slop_profile - \n",
            "Full results are available at:\n",
            "2025-04-10 09:23:48,372 - INFO - slop_profile - - Combined metrics file: /content/slop-forensics/results/slop_profile_results.json\n",
            "2025-04-10 09:23:48,372 - INFO - slop_profile - - Individual analysis files: /content/slop-forensics/results/analysis/analysis_*.json\n",
            "\n",
            "Analysis script finished.\n",
            "Check 'results/analysis/' for per-model JSON files.\n",
            "Check 'results/slop_profile_results.json' for the combined metrics.\n",
            "\n",
            "Combined metrics file location:\n",
            "-rw-r--r-- 1 root root 765399 Apr 10 09:23 results/slop_profile_results.json\n",
            "\n",
            "Individual analysis file locations:\n",
            "total 692\n",
            "-rw-r--r-- 1 root root 177605 Apr 10 09:23 slop_profile__google__gemma-3-4b-it.json\n",
            "-rw-r--r-- 1 root root 175486 Apr 10 09:23 slop_profile__meta-llama__llama-4-maverick.json\n",
            "-rw-r--r-- 1 root root 175508 Apr 10 09:23 slop_profile__meta-llama__llama-4-scout.json\n",
            "-rw-r--r-- 1 root root 175730 Apr 10 09:23 slop_profile__x-ai__grok-3-mini-beta.json\n"
          ]
        }
      ],
      "source": [
        "# @title 4. Run Workflow Step 2: Analyze Outputs & Profile Slop\n",
        "# @markdown Use `slop_profile.py` to analyze the generated datasets for metrics and features.\n",
        "\n",
        "# Print the paths where NLTK is looking for data\n",
        "print(\"NLTK data paths:\", nltk.data.path)\n",
        "\n",
        "# Verify punkt is accessible\n",
        "try:\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    print(\"Successfully loaded punkt tokenizer\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading punkt tokenizer: {e}\")\n",
        "\n",
        "print(\"Starting analysis of generated datasets...\")\n",
        "\n",
        "# Run the command (using defaults)\n",
        "analyze_command = \"python3 scripts/slop_profile.py\"\n",
        "\n",
        "print(\"\\nExecuting command:\")\n",
        "print(analyze_command)\n",
        "!{analyze_command}\n",
        "\n",
        "print(\"\\nAnalysis script finished.\")\n",
        "print(\"Check 'results/analysis/' for per-model JSON files.\")\n",
        "print(\"Check 'results/slop_profile_results.json' for the combined metrics.\")\n",
        "print(\"\\nCombined metrics file location:\")\n",
        "!ls -l results/slop_profile_results.json\n",
        "print(\"\\nIndividual analysis file locations:\")\n",
        "!ls -l results/analysis/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "5_SlopLists",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_SlopLists",
        "outputId": "ca94822e-d757-4bfe-c714-d22926090992"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting slop list creation...\n",
            "\n",
            "Executing command:\n",
            "python3 scripts/create_slop_lists.py\n",
            "2025-04-10 09:23:59,477 - INFO - create_slop_lists - Starting slop list creation from analysis files in: /content/slop-forensics/results/analysis\n",
            "2025-04-10 09:23:59,477 - INFO - create_slop_lists - Output directory: /content/slop-forensics/results/slop_lists\n",
            "2025-04-10 09:23:59,477 - INFO - slop_lists - Starting combined slop list generation...\n",
            "2025-04-10 09:23:59,477 - INFO - slop_lists - Found 4 analysis files. Loading data...\n",
            "Loading analysis files: 100% 4/4 [00:00<00:00, 147.97it/s]\n",
            "2025-04-10 09:23:59,506 - INFO - slop_lists - Processing combined text data from 4 models...\n",
            "2025-04-10 09:23:59,506 - INFO - slop_lists - Counting combined words...\n",
            "Counting words: 100% 400/400 [00:00<00:00, 1103.64it/s]\n",
            "2025-04-10 09:23:59,869 - INFO - slop_lists - Filtering combined counts...\n",
            "2025-04-10 09:23:59,896 - INFO - slop_lists - Analyzing combined word rarity...\n",
            "2025-04-10 09:24:00,273 - INFO - slop_lists - Filtering common words (wordfreq > 1.2e-05)...\n",
            "2025-04-10 09:24:00,278 - INFO - slop_lists - Finding over-represented and zero-frequency words...\n",
            "2025-04-10 09:24:00,291 - INFO - slop_lists - Creating final word slop lists...\n",
            "2025-04-10 09:24:00,296 - INFO - utils - Saved list with one item per line to: /content/slop-forensics/results/slop_lists/slop_list.json\n",
            "2025-04-10 09:24:00,296 - INFO - slop_lists - Saved standard word slop list (1500 words).\n",
            "2025-04-10 09:24:00,300 - INFO - slop_lists - Saved frequency-sorted word slop list (1500 words).\n",
            "2025-04-10 09:24:00,301 - INFO - slop_lists - Aggregating N-gram data for slop lists...\n",
            "Aggregating N-grams: 100% 4/4 [00:00<00:00, 347.41it/s]\n",
            "2025-04-10 09:24:00,313 - INFO - utils - Saved list with one item per line to: /content/slop-forensics/results/slop_lists/slop_list_bigrams.json\n",
            "2025-04-10 09:24:00,314 - INFO - slop_lists - Saved bigram slop list (151 bigrams).\n",
            "2025-04-10 09:24:00,314 - INFO - utils - Saved list with one item per line to: /content/slop-forensics/results/slop_lists/slop_list_trigrams.json\n",
            "2025-04-10 09:24:00,314 - INFO - slop_lists - Saved trigram slop list (91 trigrams).\n",
            "2025-04-10 09:24:00,314 - INFO - slop_lists - Extracting and saving slop phrases from combined data...\n",
            "2025-04-10 09:24:00,314 - INFO - slop_lists - Extracting top 1000 3-grams, then retrieving phrases...\n",
            "2025-04-10 09:24:00,314 - INFO - slop_lists - Extracting cleaned 3-grams from 400 combined texts...\n",
            "2025-04-10 09:24:03,402 - INFO - slop_lists - Found 1000 unique 3-grams after cleaning.\n",
            "2025-04-10 09:24:03,403 - INFO - slop_lists - Created set of 1000 top n-gram tuples.\n",
            "2025-04-10 09:24:03,403 - INFO - slop_lists - Spawning up to 2 worker processes for phrase extraction...\n",
            "MP substring extraction: 100% 400/400 [00:03<00:00, 131.78it/s]\n",
            "2025-04-10 09:24:06,473 - INFO - slop_lists - Merged counters: 1577 unique substrings found.\n",
            "2025-04-10 09:24:06,474 - INFO - slop_lists - After filtering, we have 1512 unique phrases.\n",
            "2025-04-10 09:24:06,487 - INFO - slop_lists - Saved phrase data to: /content/slop-forensics/results/slop_lists/slop_list_phrases.jsonl\n",
            "2025-04-10 09:24:06,487 - INFO - slop_lists - Saved top 1512 phrases to /content/slop-forensics/results/slop_lists/slop_list_phrases.jsonl.\n",
            "2025-04-10 09:24:06,489 - INFO - slop_lists - Slop list + phrase generation finished.\n",
            "2025-04-10 09:24:06,493 - INFO - create_slop_lists - Slop list creation script finished.\n",
            "\n",
            "Slop list creation script finished.\n",
            "Check 'results/slop_lists/' for the generated lists:\n",
            "total 108\n",
            "-rw-r--r-- 1 root root  2615 Apr 10 09:24 slop_list_bigrams.json\n",
            "-rw-r--r-- 1 root root 26168 Apr 10 09:24 slop_list_by_freq.json\n",
            "-rw-r--r-- 1 root root 21173 Apr 10 09:24 slop_list.json\n",
            "-rw-r--r-- 1 root root 48327 Apr 10 09:24 slop_list_phrases.jsonl\n",
            "-rw-r--r-- 1 root root  2181 Apr 10 09:24 slop_list_trigrams.json\n"
          ]
        }
      ],
      "source": [
        "# @title 5. Run Workflow Step 3: Create Slop Lists\n",
        "# @markdown Use `create_slop_lists.py` to aggregate findings and build canonical slop lists.\n",
        "\n",
        "print(\"Starting slop list creation...\")\n",
        "\n",
        "# Run the command (using defaults)\n",
        "slop_list_command = \"python3 scripts/create_slop_lists.py\"\n",
        "\n",
        "print(\"\\nExecuting command:\")\n",
        "print(slop_list_command)\n",
        "!{slop_list_command}\n",
        "\n",
        "print(\"\\nSlop list creation script finished.\")\n",
        "print(\"Check 'results/slop_lists/' for the generated lists:\")\n",
        "!ls -l results/slop_lists/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "6_PhyloTrees",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_PhyloTrees",
        "outputId": "9e802a8a-01d5-452d-f77e-797e861ad116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting phylogenetic tree generation...\n",
            "\n",
            "Executing command:\n",
            "python3 scripts/generate_phylo_trees.py\n",
            "2025-04-10 09:24:14,463 - INFO - generate_phylo_trees - Starting phylogenetic tree generation using data from: /content/slop-forensics/results/slop_profile_results.json\n",
            "2025-04-10 09:24:14,463 - INFO - generate_phylo_trees - Output directory: /content/slop-forensics/results/phylogeny\n",
            "2025-04-10 09:24:14,463 - INFO - generate_phylo_trees - Top N features per model: 1500\n",
            "2025-04-10 09:24:14,464 - INFO - phylogeny - Starting phylogenetic tree generation...\n",
            "2025-04-10 09:24:14,464 - INFO - phylogeny - Loading combined metrics data from: /content/slop-forensics/results/slop_profile_results.json\n",
            "2025-04-10 09:24:14,477 - INFO - phylogeny - Extracting features (top words/ngrams) for tree building...\n",
            "2025-04-10 09:24:14,479 - INFO - phylogeny - Attempting parsimony tree construction using PHYLIP...\n",
            "2025-04-10 09:24:14,480 - INFO - phylogeny - Parsimony analysis: 4 models, 2565 features.\n",
            "2025-04-10 09:24:14,482 - INFO - phylogeny - Using PHYLIP executables from: /usr/lib/phylip/bin\n",
            "2025-04-10 09:24:14,482 - INFO - phylogeny - Running PHYLIP 'pars'...\n",
            "2025-04-10 09:24:14,490 - INFO - phylogeny - Running PHYLIP 'consense'...\n",
            "2025-04-10 09:24:14,493 - ERROR - phylogeny - PHYLIP command 'consense' failed with exit code 255\n",
            "STDERR:\n",
            "\n",
            "2025-04-10 09:24:14,493 - ERROR - phylogeny - PHYLIP command 'consense' failed with exit code 255\n",
            "STDERR:\n",
            "\n",
            "2025-04-10 09:24:14,493 - INFO - phylogeny - Loading final tree from: /content/slop-forensics/results/phylogeny/parsimony_outtree_raw\n",
            "2025-04-10 09:24:14,494 - INFO - phylogeny - Successfully loaded and processed parsimony tree.\n",
            "2025-04-10 09:24:14,495 - INFO - phylogeny - Successfully generated parsimony tree.\n",
            "2025-04-10 09:24:14,495 - INFO - phylogeny - Rendering final parsimony tree visualizations...\n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "2025-04-10 09:24:14,711 - INFO - phylogeny - Saved basic overview tree: /content/slop-forensics/results/phylogeny/parsimony_tree_basic.png\n",
            "Rendering parsimony charts:   0% 0/4 [00:00<?, ?it/s]2025-04-10 09:24:14,823 - INFO - phylogeny - Saved C tree 'x-ai__grok-3-mini-beta__parsimony_circular.png' (highlight: x-ai/grok-3-mini-beta)\n",
            "2025-04-10 09:24:14,835 - INFO - phylogeny - Saved R tree 'x-ai__grok-3-mini-beta__parsimony_rectangular.png' (highlight: x-ai/grok-3-mini-beta)\n",
            "Rendering parsimony charts:  25% 1/4 [00:00<00:00,  8.35it/s]2025-04-10 09:24:14,933 - INFO - phylogeny - Saved C tree 'meta-llama__llama-4-scout__parsimony_circular.png' (highlight: meta-llama/llama-4-scout)\n",
            "2025-04-10 09:24:14,945 - INFO - phylogeny - Saved R tree 'meta-llama__llama-4-scout__parsimony_rectangular.png' (highlight: meta-llama/llama-4-scout)\n",
            "Rendering parsimony charts:  50% 2/4 [00:00<00:00,  8.78it/s]2025-04-10 09:24:15,033 - INFO - phylogeny - Saved C tree 'meta-llama__llama-4-maverick__parsimony_circular.png' (highlight: meta-llama/llama-4-maverick)\n",
            "2025-04-10 09:24:15,045 - INFO - phylogeny - Saved R tree 'meta-llama__llama-4-maverick__parsimony_rectangular.png' (highlight: meta-llama/llama-4-maverick)\n",
            "2025-04-10 09:24:15,136 - INFO - phylogeny - Saved C tree 'google__gemma-3-4b-it__parsimony_circular.png' (highlight: google/gemma-3-4b-it)\n",
            "2025-04-10 09:24:15,147 - INFO - phylogeny - Saved R tree 'google__gemma-3-4b-it__parsimony_rectangular.png' (highlight: google/gemma-3-4b-it)\n",
            "Rendering parsimony charts: 100% 4/4 [00:00<00:00,  9.27it/s]\n",
            "2025-04-10 09:24:15,148 - INFO - phylogeny - Saved tree in Newick format: /content/slop-forensics/results/phylogeny/parsimony_tree.nwk\n",
            "2025-04-10 09:24:15,148 - INFO - phylogeny - Saved tree in Nexus format: /content/slop-forensics/results/phylogeny/parsimony_tree.nex\n",
            "2025-04-10 09:24:15,148 - INFO - phylogeny - Phylogenetic tree generation (parsimony) completed successfully.\n",
            "2025-04-10 09:24:15,149 - INFO - generate_phylo_trees - Phylogenetic tree generation script finished.\n",
            "\n",
            "Phylogenetic tree generation script finished.\n",
            "Check 'results/phylogeny/' for tree files (Newick, Nexus) and charts:\n",
            "results/phylogeny/:\n",
            "total 64\n",
            "drwxr-xr-x 2 root root  4096 Apr 10 09:24 charts\n",
            "-rw-r--r-- 1 root root 10312 Apr 10 09:24 parsimony_infile\n",
            "-rw-r--r-- 1 root root   372 Apr 10 09:24 parsimony_model_codes.json\n",
            "-rw-r--r-- 1 root root   642 Apr 10 09:24 parsimony_outfile\n",
            "-rw-r--r-- 1 root root    64 Apr 10 09:24 parsimony_outtree_raw\n",
            "-rw-r--r-- 1 root root 24893 Apr 10 09:24 parsimony_tree_basic.png\n",
            "-rw-r--r-- 1 root root   156 Apr 10 09:24 parsimony_tree.nex\n",
            "-rw-r--r-- 1 root root   106 Apr 10 09:24 parsimony_tree.nwk\n",
            "\n",
            "results/phylogeny/charts:\n",
            "total 256\n",
            "-rw-r--r-- 1 root root 42235 Apr 10 09:24 google__gemma-3-4b-it__parsimony_circular.png\n",
            "-rw-r--r-- 1 root root 16865 Apr 10 09:24 google__gemma-3-4b-it__parsimony_rectangular.png\n",
            "-rw-r--r-- 1 root root 42979 Apr 10 09:24 meta-llama__llama-4-maverick__parsimony_circular.png\n",
            "-rw-r--r-- 1 root root 16826 Apr 10 09:24 meta-llama__llama-4-maverick__parsimony_rectangular.png\n",
            "-rw-r--r-- 1 root root 42331 Apr 10 09:24 meta-llama__llama-4-scout__parsimony_circular.png\n",
            "-rw-r--r-- 1 root root 16814 Apr 10 09:24 meta-llama__llama-4-scout__parsimony_rectangular.png\n",
            "-rw-r--r-- 1 root root 42948 Apr 10 09:24 x-ai__grok-3-mini-beta__parsimony_circular.png\n",
            "-rw-r--r-- 1 root root 16744 Apr 10 09:24 x-ai__grok-3-mini-beta__parsimony_rectangular.png\n"
          ]
        }
      ],
      "source": [
        "# @title 6. Run Workflow Step 4: Generate Phylogenetic Trees\n",
        "# @markdown Use `generate_phylo_trees.py` to cluster models based on slop profiles.\n",
        "# @markdown **Note:** This step requires PHYLIP to be installed correctly for parsimony. If PHYLIP fails, it should fall back to hierarchical clustering.\n",
        "\n",
        "print(\"Starting phylogenetic tree generation...\")\n",
        "\n",
        "# Run the command (using defaults)\n",
        "phylo_command = \"python3 scripts/generate_phylo_trees.py\"\n",
        "\n",
        "print(\"\\nExecuting command:\")\n",
        "print(phylo_command)\n",
        "!{phylo_command}\n",
        "\n",
        "print(\"\\nPhylogenetic tree generation script finished.\")\n",
        "print(\"Check 'results/phylogeny/' for tree files (Newick, Nexus) and charts:\")\n",
        "!ls -lR results/phylogeny/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7_Explore",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "id": "7_Explore",
        "outputId": "ef938b8d-0268-49db-ddde-e755e9fe137b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Exploring Results ---\n",
            "\n",
            "Looking for tree charts in: results/phylogeny/charts\n",
            "Displaying chart: results/phylogeny/charts/meta-llama__llama-4-maverick__parsimony_rectangular.png\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "image/png": {
              "width": 600
            }
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading word slop list from: results/slop_lists/slop_list.json\n",
            "Top 15 words from results/slop_lists/slop_list.json:\n",
            "- aback\n",
            "- abernathy\n",
            "- absently\n",
            "- absentmindedly\n",
            "- absurdities\n",
            "- absurdity\n",
            "- abuzz\n",
            "- accusatory\n",
            "- accustomed\n",
            "- ache\n",
            "- ached\n",
            "- acrid\n",
            "- adolescens\n",
            "- adolescentus\n",
            "- aelina\n",
            "\n",
            "Loading phrase slop list from: results/slop_lists/slop_list_phrases.jsonl\n",
            "Top 15 phrases from results/slop_lists/slop_list_phrases.jsonl:\n",
            "- \"took a deep breath\" (freq: 84)\n",
            "- \"couldn't shake the feeling\" (freq: 70)\n",
            "- \"felt a shiver run\" (freq: 53)\n",
            "- \"voice barely above a whisper\" (freq: 49)\n",
            "- \"shiver run down my spine\" (freq: 40)\n",
            "- \"couldn't help but wonder\" (freq: 39)\n",
            "- \"said, his voice low\" (freq: 36)\n",
            "- \"couldn't help but feel\" (freq: 35)\n",
            "- \"room fell silent\" (freq: 26)\n",
            "- \"voice barely a whisper\" (freq: 26)\n",
            "- \"rain continued to fall\" (freq: 26)\n",
            "- \"couldn't help but think\" (freq: 23)\n",
            "- \"words hung in the air\" (freq: 21)\n",
            "- \"heart skipped a beat\" (freq: 18)\n",
            "- \"smile playing on his lips\" (freq: 18)\n",
            "\n",
            "--- End Exploration ---\n"
          ]
        }
      ],
      "source": [
        "# @title 7. Explore Results (Optional)\n",
        "# @markdown Display some of the generated results.\n",
        "\n",
        "import json\n",
        "from IPython.display import Image, display\n",
        "import glob\n",
        "import os # Ensure os is imported\n",
        "\n",
        "print(\"--- Exploring Results ---\")\n",
        "\n",
        "# --- Display a generated tree chart ---\n",
        "chart_dir = \"results/phylogeny/charts\"\n",
        "print(f\"\\nLooking for tree charts in: {chart_dir}\")\n",
        "png_files = glob.glob(os.path.join(chart_dir, \"*.png\"))\n",
        "\n",
        "if png_files:\n",
        "    # Display the first circular PNG found\n",
        "    display_file = None\n",
        "    for f in png_files:\n",
        "        if \"circular\" in f.lower():\n",
        "             display_file = f\n",
        "             break\n",
        "    if not display_file: # Fallback to first PNG if no circular found\n",
        "        display_file = png_files[0]\n",
        "\n",
        "    print(f\"Displaying chart: {display_file}\")\n",
        "    try:\n",
        "        display(Image(filename=display_file, width=600))\n",
        "    except Exception as e:\n",
        "        print(f\"Error displaying image {display_file}: {e}\")\n",
        "else:\n",
        "    print(\"No .png charts found in the charts directory.\")\n",
        "    # Check if PHYLIP_INSTALLED was set earlier (requires running cell 1)\n",
        "    if 'PHYLIP_INSTALLED' in locals() and not PHYLIP_INSTALLED:\n",
        "        print(\"This might be because PHYLIP was not found or failed.\")\n",
        "\n",
        "# --- Display the first few items from the word slop list ---\n",
        "slop_list_file = \"results/slop_lists/slop_list.json\"\n",
        "print(f\"\\nLoading word slop list from: {slop_list_file}\")\n",
        "\n",
        "try:\n",
        "    with open(slop_list_file, 'r') as f:\n",
        "        slop_data = json.load(f)\n",
        "    print(f\"Top 15 words from {slop_list_file}:\")\n",
        "    # Data is list of lists, e.g., [[\"word1\"], [\"word2\"]]\n",
        "    for item in slop_data[:15]:\n",
        "        print(f\"- {item[0]}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"File not found: {slop_list_file}\")\n",
        "except (json.JSONDecodeError, IndexError, TypeError) as e:\n",
        "    print(f\"Error reading or parsing {slop_list_file}: {e}\")\n",
        "\n",
        "\n",
        "# --- Display the first few items from the phrase slop list ---\n",
        "phrase_list_file = \"results/slop_lists/slop_list_phrases.jsonl\"\n",
        "print(f\"\\nLoading phrase slop list from: {phrase_list_file}\")\n",
        "\n",
        "try:\n",
        "    print(f\"Top 15 phrases from {phrase_list_file}:\")\n",
        "    count = 0\n",
        "    with open(phrase_list_file, 'r') as f:\n",
        "        for line in f:\n",
        "            if count >= 15:\n",
        "                break\n",
        "            try:\n",
        "                # Data is JSONL: [phrase, frequency] per line\n",
        "                phrase_data = json.loads(line)\n",
        "                print(f\"- \\\"{phrase_data[0]}\\\" (freq: {phrase_data[1]})\")\n",
        "                count += 1\n",
        "            except (json.JSONDecodeError, IndexError, TypeError) as e:\n",
        "                print(f\"  Skipping invalid line: {line.strip()} ({e})\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"File not found: {phrase_list_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error reading {phrase_list_file}: {e}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- End Exploration ---\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
